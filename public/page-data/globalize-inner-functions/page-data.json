{"componentChunkName":"component---src-pages-markdown-remark-fields-slug-js","path":"/globalize-inner-functions/","result":{"data":{"markdownRemark":{"html":"<p>In the last month, I faced an interesting problems using Python that shed light on an interesting aspect of this language.</p>\n<h2 id=\"the-use-case\" style=\"position:relative;\"><a href=\"#the-use-case\" aria-label=\"the use case permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The use case</h2>\n<p>At Sunrise I worked with sleep recordings to detect apneas and respiratory efforts using machine learning models. Some of our data pipelines exploit the predictions to generate some useful clinical socres that permits to evaluate the quality of the night. Regularly and to follow up and the quality of our models, we caclulate the scores from a subsequent portion of our historical nights recorded with our medical device and their respective labeled data from classical pletysmography. The pipeline is deployed on Vertex.ai and is articulated in several steps:</p>\n<ul>\n<li>Gather all labeled data (hypnograms, respiratory events, head motion)</li>\n<li>Do some processing of the labeled data (column naming, mapping of the labels) and concatenate all nights</li>\n<li>Calculate scores from labeled data and our predictions</li>\n<li>Generate a full report comparing and evaluating the scores from our predictions and lebeled data.</li>\n</ul>\n<p>The first step was quite long and intensive. The algorithm for processing was loading one by one each <code>csv</code> file containing the labeled data and concatenating all into one big dataframe. We had 2200 nights with files with almost 100000 lines each. It was roughly taking 30 to 60 minutes to process our different files.</p>\n<h2 id=\"the-code\" style=\"position:relative;\"><a href=\"#the-code\" aria-label=\"the code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The code</h2>\n<p>The processing step was implemented as a <a href=\"https://fbraza.github.io/fbraza-github-pages/overview-of-vertexai/\">Vertex.ai</a> pipeline's step. It was very similar to the following:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token decorator annotation punctuation\">@component</span><span class=\"token punctuation\">(</span>\n    packages_to_install<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"any\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"library\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"you\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    base_image<span class=\"token operator\">=</span><span class=\"token string\">\"uri/to/image\"</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">concatenate_hypnograms</span><span class=\"token punctuation\">(</span>\n    labels_path<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> concatenated_hypnos<span class=\"token punctuation\">:</span> Output<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">from</span> datetime <span class=\"token keyword\">import</span> datetime\n\n    <span class=\"token keyword\">import</span> pandas\n    <span class=\"token keyword\">import</span> os\n\n    nights_hypnograms <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>listdir<span class=\"token punctuation\">(</span><span class=\"token string\">\"path/to/hypnograms/csv\"</span><span class=\"token punctuation\">)</span>\n    dataframes <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> night <span class=\"token keyword\">in</span> nights_hypnograms<span class=\"token punctuation\">:</span>\n      df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"parent/</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>night<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n      <span class=\"token comment\"># Do other things</span>\n      dataframes<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">)</span>\n\n    final_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>concatenate<span class=\"token punctuation\">(</span>dataframes<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>It is easy to understand that the code is running at O(n) and that time complexity will increase with the number of nights. To speed up the process I investigated the use of multiprocessing and start implementing the code:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> multiprocessing <span class=\"token keyword\">import</span> Pool\n\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">parralel_read_csv_for_labels</span><span class=\"token punctuation\">(</span>labels_path<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">:</span>\n    file_list <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n        name\n        <span class=\"token keyword\">for</span> name <span class=\"token keyword\">in</span> glob<span class=\"token punctuation\">.</span>glob<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>labels_path<span class=\"token punctuation\">}</span></span><span class=\"token string\">/**\"</span></span><span class=\"token punctuation\">,</span> recursive<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">}</span>\n\n    <span class=\"token keyword\">with</span> Pool<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> pool<span class=\"token punctuation\">:</span>\n        df_list <span class=\"token operator\">=</span> pool<span class=\"token punctuation\">.</span>starmap<span class=\"token punctuation\">(</span>\n            add_night_column_for_labels<span class=\"token punctuation\">,</span>\n            <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>file_list<span class=\"token punctuation\">,</span> itertools<span class=\"token punctuation\">.</span>repeat<span class=\"token punctuation\">(</span>sunalgo_labels_path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">)</span>\n        combined_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span>df_list<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> combined_df</code></pre></div>\n<p>A code very similar to this one, was implemented in our in-house library. We could then just import it in our component:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token decorator annotation punctuation\">@component</span><span class=\"token punctuation\">(</span>\n    packages_to_install<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"any\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"library\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"you\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    base_image<span class=\"token operator\">=</span><span class=\"token string\">\"uri/to/image\"</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">concatenate_hypnograms</span><span class=\"token punctuation\">(</span>\n    labels_path<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> concatenated_hypnos<span class=\"token punctuation\">:</span> Output<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">from</span> datetime <span class=\"token keyword\">import</span> datetime\n\n    <span class=\"token keyword\">import</span> pandas\n    <span class=\"token keyword\">import</span> os\n    <span class=\"token keyword\">from</span> library <span class=\"token keyword\">import</span> parralel_read_csv_for_labels\n\n    final_df <span class=\"token operator\">=</span> parralel_read_csv_for_labels<span class=\"token punctuation\">(</span>labels_path<span class=\"token operator\">=</span>labels_path<span class=\"token punctuation\">)</span></code></pre></div>\n<p>Executing our code returned the following error:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">_pickle.PicklingError: Can<span class=\"token string\">'t pickle &lt;class '</span>_thread.lock'<span class=\"token operator\">></span>: attribute lookup lock on _thread failed</code></pre></div>\n<p>This error is due to the fact that importing the function inside my component changed its scope. Indeed, <code>multiprocessing</code> uses pickle to package a function in order to send it to other workers, but it doesn't work for non-top-level functions. The trick was to make beleive the workers that the functions was actually at the top level of the scope. To do that I used the following decorator:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> sys\n<span class=\"token keyword\">import</span> uuid\n<span class=\"token keyword\">from</span> functools <span class=\"token keyword\">import</span> wraps\n\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">globalize</span><span class=\"token punctuation\">(</span>func<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"\n    Function defined to reset the scope hierachy and make an inner function\n    considered as a top-level function. We need this in order to use multiprocess\n    Pool to parallelize tasks with inner functions in our kubeflow components.\n    \"\"\"</span>\n\n    <span class=\"token decorator annotation punctuation\">@wraps</span><span class=\"token punctuation\">(</span>func<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">result</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> func<span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span>\n\n    result<span class=\"token punctuation\">.</span>__name__ <span class=\"token operator\">=</span> result<span class=\"token punctuation\">.</span>__qualname__ <span class=\"token operator\">=</span> uuid<span class=\"token punctuation\">.</span>uuid4<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">hex</span>\n    <span class=\"token builtin\">setattr</span><span class=\"token punctuation\">(</span>sys<span class=\"token punctuation\">.</span>modules<span class=\"token punctuation\">[</span>result<span class=\"token punctuation\">.</span>__module__<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> result<span class=\"token punctuation\">.</span>__name__<span class=\"token punctuation\">,</span> result<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> result</code></pre></div>\n<p><code>globalize</code> effectively clones the function, gives the clone a unique name, and inserts the clone as a top-level function into the original function's module. The nice thing is that the clone retains the original function's context, allowing it to access variables that the original function can. By refactoring the previous code we could speed up our pipeline steps going from 30-60 minutes to 5-10 minutes processing time.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token decorator annotation punctuation\">@component</span><span class=\"token punctuation\">(</span>\n    packages_to_install<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"any\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"library\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"you\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    base_image<span class=\"token operator\">=</span><span class=\"token string\">\"uri/to/image\"</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">concatenate_hypnograms</span><span class=\"token punctuation\">(</span>\n    labels_path<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> concatenated_hypnos<span class=\"token punctuation\">:</span> Output<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">from</span> datetime <span class=\"token keyword\">import</span> datetime\n\n    <span class=\"token keyword\">import</span> pandas\n    <span class=\"token keyword\">import</span> os\n    <span class=\"token keyword\">from</span> library <span class=\"token keyword\">import</span> parralel_read_csv_for_labels<span class=\"token punctuation\">,</span> globalize\n\n    parralel_read_csv_for_labels <span class=\"token operator\">=</span> globalize<span class=\"token punctuation\">(</span>parralel_read_csv_for_labels<span class=\"token punctuation\">)</span>\n    final_df <span class=\"token operator\">=</span> parralel_read_csv_for_labels<span class=\"token punctuation\">(</span>labels_path<span class=\"token operator\">=</span>labels_path<span class=\"token punctuation\">)</span></code></pre></div>\n<h2 id=\"final-thoughts\" style=\"position:relative;\"><a href=\"#final-thoughts\" aria-label=\"final thoughts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Final thoughts</h2>\n<p>The primary objective was to speed up this processing step. Using this approach worked effciently. There is another approach that was not investigated and could be more elegant: the <code>[ParallelFor](https://kubeflow-pipelines.readthedocs.io/en/sdk-2.2.0/source/dsl.html?h=parallelfor#kfp.dsl.ParallelFor)</code> class provided by the kubeflow sdk. This would imply refactoring the code to exclude any concatenation and treat all files as an indepednant set of data and aggregate everything at the scores computations and reporting steps.</p>\n<h2 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h2>\n<ul>\n<li><a href=\"https://kubeflow-pipelines.readthedocs.io/en/sdk-2.2.0/index.html\">https://kubeflow-pipelines.readthedocs.io/en/sdk-2.2.0/index.html</a></li>\n<li><a href=\"https://docs.python.org/3/library/multiprocessing.html\">https://docs.python.org/3/library/multiprocessing.html</a></li>\n<li><a href=\"https://realpython.com/primer-on-python-decorators/\">https://realpython.com/primer-on-python-decorators/</a></li>\n</ul>","frontmatter":{"title":"Globalize the scope of a python function","date":"August 23, 2023","tags":["Python"]}}},"pageContext":{"id":"5694cdf9-5833-5d34-a7b4-f00c3dacc921","fields__slug":"/globalize-inner-functions/","__params":{"fields__slug":"globalize-inner-functions"}}},"staticQueryHashes":["2750728228"],"slicesMap":{}}