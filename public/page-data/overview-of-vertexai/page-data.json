{"componentChunkName":"component---src-pages-markdown-remark-fields-slug-js","path":"/overview-of-vertexai/","result":{"data":{"markdownRemark":{"html":"<p><a href=\"https://cloud.google.com/vertex-ai?hl=us\">Vertex.ai</a> is a Google Cloud service that enables the user to build, deploy, and scale data pipelines and machine learning models. It includes some services to track and manage model artifacts, performances and metrics. Google provide a <a href=\"https://cloud.google.com/vertex-ai/docs/start/use-vertex-ai-python-sdk\">Python SDK</a> that permits the user to interact directly with the different services.</p>\n<p>Vertex AI pipeline allows you to orchestrate different steps of a data pipeline. Each step of the pipeline is an autonomous piece of code that will be executed in its own container. Each of these steps can accept inputs and generate outputs for downstream steps. Altogether the steps define a workflow that can be represented as an directed acyclic graph (DAG) (Figure 1). Here I am going to gather some personal notes and feedbacks related to my own experience with this cloud service and try to provide some personal advises.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1024px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/fbraza-github-pages/static/f4801f88256f9bf9569609d1acbf8b82/29114/2023-05-06-overview-vertexai-figure-01.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 56.25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAByElEQVR42n1S2U4VQRDt/3/hQYOoEEP8Bn0AhUCIH2LEmLmz9b73oapnwBtM7ORMdae2U6dGnN+MuLxf8PmHwbtvE06+PBJ+483XXzi7esSH6z+4uBnw6U7i/a3Ex9sZp98lLu41zu803l6NOLs+4PJBkf0JUZIBakA/LaHxvUXU7Pd7Jkdh5z9orSIFB28WquGRk4cY1ghpM3xsKLXBxopFOVgXsDiF2ZCf3qvWGPSKRfPbYLUWizcwxiHE1OONdRDOB6SUqVhF3ZFihnMJpWTkUhBCgnfbvRD4zjmVGFpj4b2HlLJDKKVwOBx6MJ9cKhWIW8cYYEOmzp4aOChqrv3+Dh6xRBozIecMTRNwrV7QGEN6kCrEzqcCpR2Y+WwVlIsU7CApYXUampgrGlkaDUc6z/Pc853z3Qr+hLAthcdlsRPpyEyjpxFzofEKsajUtFFMQ4x1j0VnxvnTNGIcRwhL4jJLDuAEBhebladgQ0vQmIgNL2zT8K/WfDiXi2pLk5Ceomuj5EvQhtbFb7X1d6llZ1dfGj8X5EUMwwBPujNTcRz0jOOk142OfWylVLuGlvScIFJKGxty8vpfF/8/gHWlf3NZOju2T9fLT5tjaaU1AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Vertexai\" title=\"Vertexai\" src=\"/fbraza-github-pages/static/f4801f88256f9bf9569609d1acbf8b82/2bef9/2023-05-06-overview-vertexai-figure-01.png\" srcset=\"/fbraza-github-pages/static/f4801f88256f9bf9569609d1acbf8b82/6f3f2/2023-05-06-overview-vertexai-figure-01.png 256w,\n/fbraza-github-pages/static/f4801f88256f9bf9569609d1acbf8b82/01e7c/2023-05-06-overview-vertexai-figure-01.png 512w,\n/fbraza-github-pages/static/f4801f88256f9bf9569609d1acbf8b82/2bef9/2023-05-06-overview-vertexai-figure-01.png 1024w,\n/fbraza-github-pages/static/f4801f88256f9bf9569609d1acbf8b82/71c1d/2023-05-06-overview-vertexai-figure-01.png 1536w,\n/fbraza-github-pages/static/f4801f88256f9bf9569609d1acbf8b82/29114/2023-05-06-overview-vertexai-figure-01.png 1920w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span></p>\n<p><em>Figure 1</em> : Example of a vertex ai pipeline</p>\n<h2 id=\"anatomy-of-a-component\" style=\"position:relative;\"><a href=\"#anatomy-of-a-component\" aria-label=\"anatomy of a component permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Anatomy of a component</h2>\n<p>The core unit of computation is called a component. A component is an independent piece of code that will be uploaded and containarized in the Vertex platform to be executed. With Vertex, you can use the <a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">Kubeflow pipelines sdk</a> to implement your code. A typical components can be implemented as followed.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> kfp<span class=\"token punctuation\">.</span>v2<span class=\"token punctuation\">.</span>dsl <span class=\"token keyword\">import</span> <span class=\"token punctuation\">(</span>\n    Dataset<span class=\"token punctuation\">,</span>\n    Input<span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token decorator annotation punctuation\">@component</span><span class=\"token punctuation\">(</span>\n    packages_to_install<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"put\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"here\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"librairies\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"to\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"install\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    base_image<span class=\"token operator\">=</span><span class=\"token string\">\"the docker image to use\"</span><span class=\"token punctuation\">,</span>\n    output_component_file<span class=\"token operator\">=</span><span class=\"token string\">\"component_one.yaml\"</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">component_one</span><span class=\"token punctuation\">(</span>processed_data<span class=\"token punctuation\">:</span> Output<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># ---- imports</span>\n    <span class=\"token comment\"># Import your modules</span>\n\n    <span class=\"token comment\"># ---- Helpers</span>\n    <span class=\"token comment\"># Implemenent helpers functions</span>\n\n    <span class=\"token comment\"># ---- variable</span>\n    <span class=\"token comment\"># add any local variable</span>\n\n    <span class=\"token comment\"># ---- Component execution</span>\n    <span class=\"token comment\"># Implement the logic of your component</span></code></pre></div>\n<p>As you can see, a component is simply a function encapsulated in a <code>@component</code> decorator. The decorator is using three arguments here:</p>\n<ul>\n<li><code>packages_to_install</code>: a <code>list</code> containing the name of the libraries you wish to install and that will be used in you component.</li>\n<li><code>base_image</code>: a docker image with the basic environment you wish to execute your code on.</li>\n<li><code>output_component_file</code>: path where you want the library to generate a <code>yaml</code> template of your component that will be interpreted by vertex to execute your task. An example is showed in the supplemental data of this article.</li>\n</ul>\n<h2 id=\"the-art-of-writing-component\" style=\"position:relative;\"><a href=\"#the-art-of-writing-component\" aria-label=\"the art of writing component permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The art of writing component</h2>\n<p>Being an independent piece of code comes with an important trade-off. <strong>Every extra code you need, has to be implemented in the component function</strong>. Your <code>import</code> statements must be declared here and any helper functions should be implemented as inner functions if you wish to use them. Any code outside the component will be simply ignored.</p>\n<p>At the beginning my components were just huge chunks of code and this was bad. Indeed, inner functions cannot be unit tested which make the maintenance of such code very difficult. To avoid that and after having implemented working components, I refactored the code such that all helpers functions were packaged in our internal library deployed in our private repository. As such these functions can be all unit tested and we can use them by pointing the <code>base_image</code> to a docker image that contains our library. Doing this dramatically reduced the size of our components and make them easy to read and maintain.</p>\n<p>Organising your code may helped to. Here the repository structure:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token builtin class-name\">.</span>\n├── ai_vertex\n│   ├── components\n│   ├── __init__.py\n│   ├── pipelines.py\n│   ├── image1.Dockerfile\n│   ├── image2.Dockerfile\n│   ├── templates\n│   └── cli.py\n</code></pre></div>\n<p>Components are stored in a <code>components</code> folder and are grouped by task (etl, ml or viz). The <code>pipelines.py</code> is where the DAGs are defined. The <code>templates</code> folder is where the templates will be generated and loaded from by the library to deploy and execute your components. I like to encaspulate my code logic into command line applications (CLI). I use <a href=\"https://typer.tiangolo.com/\">Typer</a>, a library that capitalises on <a href=\"https://click.palletsprojects.com/en/8.1.x/\">click</a> but brings the programming philosophy and simplicity of <a href=\"https://fastapi.tiangolo.com/\">FastAPI</a>. I like to use CLI, because it makes the developer experience easier and allow me to encapsulate complex tasks into simple-to-use command lines. In my case, the cli contains all the commands needed to deploy our different pipelines for model training, model evaluation and metrics report.</p>\n<h2 id=\"making-components-communicate\" style=\"position:relative;\"><a href=\"#making-components-communicate\" aria-label=\"making components communicate permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Making components communicate</h2>\n<p>Let's update our previous code.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> kfp<span class=\"token punctuation\">.</span>v2<span class=\"token punctuation\">.</span>dsl <span class=\"token keyword\">import</span> <span class=\"token punctuation\">(</span>\n    Dataset<span class=\"token punctuation\">,</span>\n    Input<span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token decorator annotation punctuation\">@component</span><span class=\"token punctuation\">(</span>\n    packages_to_install<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"put\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"here\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"librairies\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"to\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"install\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    base_image<span class=\"token operator\">=</span><span class=\"token string\">\"the docker image to use\"</span><span class=\"token punctuation\">,</span>\n    output_component_file<span class=\"token operator\">=</span><span class=\"token string\">\"component_one.yaml\"</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">component_one</span><span class=\"token punctuation\">(</span>raw_data<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> processed_data<span class=\"token punctuation\">:</span> Output<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># ---- imports</span>\n    <span class=\"token comment\"># Import your modules</span>\n\n    <span class=\"token comment\"># No helpers because we refactored :-)</span>\n\n    <span class=\"token comment\"># ---- Component execution</span>\n    <span class=\"token comment\"># Implement the logic of your component</span>\n\n<span class=\"token decorator annotation punctuation\">@component</span><span class=\"token punctuation\">(</span>\n    packages_to_install<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"put\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"here\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"librairies\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"to\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"install\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    base_image<span class=\"token operator\">=</span><span class=\"token string\">\"the docker image to use\"</span><span class=\"token punctuation\">,</span>\n    output_component_file<span class=\"token operator\">=</span><span class=\"token string\">\"component_two.yaml\"</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">component_two</span><span class=\"token punctuation\">(</span>processed_data<span class=\"token punctuation\">:</span> Input<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> metrics<span class=\"token punctuation\">:</span> Output<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># ---- imports</span>\n    <span class=\"token comment\"># Import your modules</span>\n\n    <span class=\"token comment\"># No helpers because we refactored :-)</span>\n\n    <span class=\"token comment\"># ---- Component execution</span>\n    <span class=\"token comment\"># Implement the logic of your component</span></code></pre></div>\n<p>Let's understand how a component can generate outputs / artifacts and how these can be digested in downstream steps. Let's imagine the <code>component_one</code> to be a classical processing function that takes raw data, processes it and returns an output ready to be processed by the next step. The <code>raw_data</code> argument is a string that is a path pointing to the source data, a <code>csv</code> file seated in our bucket for example. For I/O operations, you have two choices in Vertex:</p>\n<ul>\n<li>You can use the classical python <a href=\"https://cloud.google.com/python/docs/reference/storage/latest\">library</a> to interact with this file on Cloud storage</li>\n<li>You can leverage the power of <a href=\"https://cloud.google.com/storage/docs/gcs-fuse?hl=fr\">Cloud storage FUSE</a>. Briefly, FUSE is an abstraction layer on top of Cloud storage that makes python beleive, it is a simple filesystem. As such you can use classic python code to make I/O operations. Let's say you need to save a dataframe as a <code>csv</code> file:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># WITH THE LIBRARY</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">component_one</span><span class=\"token punctuation\">(</span>raw_data<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> processed_data<span class=\"token punctuation\">:</span> Output<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n    <span class=\"token keyword\">from</span> google<span class=\"token punctuation\">.</span>cloud<span class=\"token punctuation\">.</span>storage <span class=\"token keyword\">import</span> Client<span class=\"token punctuation\">,</span> Bucket\n\n    gcs_client <span class=\"token operator\">=</span> Client<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    bucket<span class=\"token punctuation\">:</span> Bucket <span class=\"token operator\">=</span> gcs_client<span class=\"token punctuation\">.</span>bucket<span class=\"token punctuation\">(</span>bucket_name<span class=\"token punctuation\">)</span>\n    blob <span class=\"token operator\">=</span> bucket<span class=\"token punctuation\">.</span>get_blob<span class=\"token punctuation\">(</span>raw_data<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Read data</span>\n    my_dataframe <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>\n        io<span class=\"token punctuation\">.</span>StringIO<span class=\"token punctuation\">(</span>blob<span class=\"token punctuation\">.</span>download_as_bytes<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>encoding<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># save the dataframe as csv</span>\n    bucket<span class=\"token punctuation\">.</span>blob<span class=\"token punctuation\">(</span>processed_data<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>upload_from_string<span class=\"token punctuation\">(</span>\n        my_dataframe<span class=\"token punctuation\">.</span>to_csv<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> content_type<span class=\"token operator\">=</span><span class=\"token string\">\"text/csv\"</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># WITH FUSE</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">component_one</span><span class=\"token punctuation\">(</span>raw_data<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> processed_data<span class=\"token punctuation\">:</span> Output<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    my_dataframe <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span><span class=\"token string\">\"path_to_data\"</span><span class=\"token punctuation\">)</span>\n    my_dataframe<span class=\"token punctuation\">.</span>to_csv<span class=\"token punctuation\">(</span><span class=\"token string\">\"path_destination\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>From a developer perspective using FUSE is really neat.</p>\n</blockquote>\n<p>Notice the type of <code>processed_data</code>. It is an <code>Output</code> of type <code>Dataset</code>. <code>Dataset</code> inherits from the <code>Artifact</code> class in the python sdk. It means that outputs are treated as artifacts (files, models, markdown, images). You can read <a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/data-types/artifacts/\">here</a> and <a href=\"https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Artifact\">here</a> for more details. An artifact holds the representation of an <code>Input</code> or <code>Output</code> object. It contains some interesting properties that allow the developer to manipulate it and extract its path (simple, use the <code>path</code> property as shown in the code example).</p>\n<p>Let's implement the second component.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">component_two</span><span class=\"token punctuation\">(</span>processed_data<span class=\"token punctuation\">:</span> Input<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> metrics<span class=\"token punctuation\">:</span> Output<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\n    data <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>processed_data<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># run your model !! Fake code !!</span>\n    model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\n    metrics_df <span class=\"token operator\">=</span> compute_metrics<span class=\"token punctuation\">(</span>y_train<span class=\"token punctuation\">,</span> y_pred_from_train<span class=\"token punctuation\">)</span>\n    metrics_df<span class=\"token punctuation\">.</span>to_csv<span class=\"token punctuation\">(</span>metrics<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">)</span></code></pre></div>\n<p>Now that we have our components we need to stick them together in a pipeline.</p>\n<h2 id=\"a-pipeline-to-rule-them-all\" style=\"position:relative;\"><a href=\"#a-pipeline-to-rule-them-all\" aria-label=\"a pipeline to rule them all permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>A pipeline to rule them all</h2>\n<p>Here the anatomy of our pipeline:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> kfp<span class=\"token punctuation\">.</span>v2<span class=\"token punctuation\">.</span>dsl <span class=\"token keyword\">import</span> pipeline\n\n<span class=\"token decorator annotation punctuation\">@pipeline</span><span class=\"token punctuation\">(</span>\n    pipeline_root<span class=\"token operator\">=</span>PIPELINE_ROOT<span class=\"token punctuation\">,</span>\n    name<span class=\"token operator\">=</span><span class=\"token string\">\"eval-prod-sleep-stage-pipeline\"</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">super_pipeline</span><span class=\"token punctuation\">(</span>\n    raw_data_path<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    process_data_task <span class=\"token operator\">=</span> component_one<span class=\"token punctuation\">(</span>raw_data<span class=\"token operator\">=</span>raw_data_path<span class=\"token punctuation\">)</span>\n    train_model_task <span class=\"token operator\">=</span> component_two<span class=\"token punctuation\">(</span>\n        processed_data<span class=\"token operator\">=</span>component_one<span class=\"token punctuation\">.</span>outputs<span class=\"token punctuation\">[</span><span class=\"token string\">\"processed_data\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span></code></pre></div>\n<blockquote>\n<p>The <code>pipeline_root</code> is the location where the pipeline will save runs and their respective artifacts in your cloud environment (buckets).</p>\n</blockquote>\n<p>The magic resides in the possibily of querying outputs of the previous task simply using the property <code>outputs</code> and the function argument name of type <code>Output[Dataset]</code>. This approach is valid for any outputs that needs to be ingested by another step.</p>\n<p>Note that here, if one of your task is resource intensive, you can actually specify your needs. Let's imagine that our training needs a lot of memory and cpus.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> kfp<span class=\"token punctuation\">.</span>v2<span class=\"token punctuation\">.</span>dsl <span class=\"token keyword\">import</span> pipeline\n\n<span class=\"token decorator annotation punctuation\">@pipeline</span><span class=\"token punctuation\">(</span>\n    pipeline_root<span class=\"token operator\">=</span>PIPELINE_ROOT<span class=\"token punctuation\">,</span>\n    name<span class=\"token operator\">=</span><span class=\"token string\">\"eval-prod-sleep-stage-pipeline\"</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">super_pipeline</span><span class=\"token punctuation\">(</span>\n    raw_data_path<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    process_data_task <span class=\"token operator\">=</span> component_one<span class=\"token punctuation\">(</span>raw_data<span class=\"token operator\">=</span>raw_data_path<span class=\"token punctuation\">)</span>\n    train_model_task <span class=\"token operator\">=</span> component_two<span class=\"token punctuation\">(</span>\n        processed_data<span class=\"token operator\">=</span>component_one<span class=\"token punctuation\">.</span>outputs<span class=\"token punctuation\">[</span><span class=\"token string\">\"processed_data\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>set_cpu_limit<span class=\"token punctuation\">(</span><span class=\"token string\">\"32\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>set_memory_limit<span class=\"token punctuation\">(</span><span class=\"token string\">\"128G\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Using the functions <code>set_cpu_limit()</code> and <code>set_memory_limit()</code> allows to do that very easily. Be careful to know the compatible configurations in your Google cloud environment.</p>\n<h2 id=\"execute-the-pipeline\" style=\"position:relative;\"><a href=\"#execute-the-pipeline\" aria-label=\"execute the pipeline permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Execute the pipeline</h2>\n<p>To execute the pipeline we use the <a href=\"https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform\">aiplatform</a> python package. It is pretty simple to implement:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> google<span class=\"token punctuation\">.</span>cloud <span class=\"token keyword\">import</span> aiplatform\n<span class=\"token keyword\">from</span> kfp<span class=\"token punctuation\">.</span>v2 <span class=\"token keyword\">import</span> compiler\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">init_pipeline</span><span class=\"token punctuation\">(</span>\n    pipeline_function<span class=\"token punctuation\">:</span> Callable<span class=\"token punctuation\">,</span>\n    params<span class=\"token punctuation\">:</span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> Any<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    template_path<span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    compiler<span class=\"token punctuation\">.</span>Compiler<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>\n        pipeline_func<span class=\"token operator\">=</span>pipeline_function<span class=\"token punctuation\">,</span>\n        pipeline_parameters<span class=\"token operator\">=</span>params<span class=\"token punctuation\">,</span>\n        package_path<span class=\"token operator\">=</span>template_path<span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n    aiplatform<span class=\"token punctuation\">.</span>init<span class=\"token punctuation\">(</span>\n        project<span class=\"token operator\">=</span>project<span class=\"token punctuation\">,</span>\n        staging_bucket<span class=\"token operator\">=</span>params<span class=\"token punctuation\">[</span><span class=\"token string\">\"staging_bucket\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        experiment<span class=\"token operator\">=</span>params<span class=\"token punctuation\">[</span><span class=\"token string\">\"experiment_name\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        location<span class=\"token operator\">=</span>location<span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> aiplatform<span class=\"token punctuation\">.</span>PipelineJob<span class=\"token punctuation\">(</span>\n        display_name<span class=\"token operator\">=</span>params<span class=\"token punctuation\">[</span><span class=\"token string\">\"experiment_name\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        template_path<span class=\"token operator\">=</span>template_path<span class=\"token punctuation\">,</span>\n        job_id<span class=\"token operator\">=</span><span class=\"token string-interpolation\"><span class=\"token string\">f'</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>prefix<span class=\"token punctuation\">}</span></span><span class=\"token string\">-</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>params<span class=\"token punctuation\">[</span><span class=\"token string\">\"experiment_name\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">-</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>params<span class=\"token punctuation\">[</span><span class=\"token string\">\"experiment_run\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">-</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>TIMESTAMP<span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">,</span>\n        enable_caching<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n\nrun <span class=\"token operator\">=</span> init_pipeline<span class=\"token punctuation\">(</span>\n    pipeline_function<span class=\"token operator\">=</span>super_pipeline<span class=\"token punctuation\">,</span>\n    params<span class=\"token operator\">=</span>params<span class=\"token punctuation\">,</span>\n    template_path<span class=\"token operator\">=</span><span class=\"token string\">\"templates/super-pipeline-template.path\"</span><span class=\"token punctuation\">,</span>\n    project<span class=\"token operator\">=</span>gcp_project<span class=\"token punctuation\">,</span>\n    location<span class=\"token operator\">=</span>gcp_location<span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\nrun<span class=\"token punctuation\">.</span>submit<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>In my code, I created an <code>init_pipeline()</code> function that allows me to instantiate and deploy several pipelines throughout my cli. The steps are quite clear here. First, you compile the code to generate your templates. Next, you use the <code>init</code> function to set the project from the cloud environment and its respective credentials that are stored as instance attributes. Finally, you instatiate a <code>PipelineJob</code> that holds the necessary metadata and templates to execute your task. Once done you can submit your task to vertex.ai</p>\n<h2 id=\"one-more-thing\" style=\"position:relative;\"><a href=\"#one-more-thing\" aria-label=\"one more thing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>One more thing</h2>\n<p>Before closing this article I'd like to mention the MLOps capacibilities that vertex offers to users. These are minimal but extremely useful. Indeed, if you need to track models metrics and parameters at each run, the <code>aiplatform</code> library provides some interesting functions. Let's modify our <code>component_two</code> accordingly:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">component_two</span><span class=\"token punctuation\">(</span>processed_data<span class=\"token punctuation\">:</span> Input<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> metrics<span class=\"token punctuation\">:</span> Output<span class=\"token punctuation\">[</span>Dataset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">compute_metrics</span><span class=\"token punctuation\">(</span>label<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    acc <span class=\"token operator\">=</span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span><span class=\"token number\">100</span> <span class=\"token operator\">*</span> metrics<span class=\"token punctuation\">.</span>accuracy_score<span class=\"token punctuation\">(</span>label<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n    bacc <span class=\"token operator\">=</span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span><span class=\"token number\">100</span> <span class=\"token operator\">*</span> metrics<span class=\"token punctuation\">.</span>balanced_accuracy_score<span class=\"token punctuation\">(</span>label<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n    kapa <span class=\"token operator\">=</span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span><span class=\"token number\">100</span> <span class=\"token operator\">*</span> metrics<span class=\"token punctuation\">.</span>cohen_kappa_score<span class=\"token punctuation\">(</span>label<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> acc<span class=\"token punctuation\">,</span> bacc<span class=\"token punctuation\">,</span> kapa\n\n  data <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>processed_data<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">)</span>\n\n  aiplatform<span class=\"token punctuation\">.</span>init<span class=\"token punctuation\">(</span>\n    project<span class=\"token operator\">=</span><span class=\"token string\">\"sensav2\"</span><span class=\"token punctuation\">,</span>\n    staging_bucket<span class=\"token operator\">=</span>staging_bucket<span class=\"token punctuation\">,</span>\n    experiment<span class=\"token operator\">=</span>experiment_name<span class=\"token punctuation\">,</span>\n    location<span class=\"token operator\">=</span><span class=\"token string\">\"europe-west1\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">with</span> aiplatform<span class=\"token punctuation\">.</span>start_run<span class=\"token punctuation\">(</span>run<span class=\"token operator\">=</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"prefix-you-can-choose-</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>experiment_run<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> run<span class=\"token punctuation\">:</span>\n    model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>model_params<span class=\"token punctuation\">)</span>\n\n    y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\n    acc<span class=\"token punctuation\">,</span> bacc<span class=\"token punctuation\">,</span> kapa <span class=\"token operator\">=</span> compute_metrics<span class=\"token punctuation\">(</span>y_train<span class=\"token punctuation\">,</span> y_pred_from_train<span class=\"token punctuation\">)</span>\n\n    metrics <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n      <span class=\"token string\">\"accuracy\"</span><span class=\"token punctuation\">:</span> acc<span class=\"token punctuation\">,</span>\n      <span class=\"token string\">\"balanced_accuracy\"</span><span class=\"token punctuation\">:</span> bacc<span class=\"token punctuation\">,</span>\n      <span class=\"token string\">\"kappa\"</span><span class=\"token punctuation\">:</span> kapa\n    <span class=\"token punctuation\">}</span>\n\n    <span class=\"token comment\"># run metrics and model parameters</span>\n    run<span class=\"token punctuation\">.</span>log_metrics<span class=\"token punctuation\">(</span>metrics_to_log_for_run<span class=\"token punctuation\">)</span>\n    run<span class=\"token punctuation\">.</span>log_params<span class=\"token punctuation\">(</span>model_params<span class=\"token punctuation\">)</span></code></pre></div>\n<p>First you need to use the <code>aiplatform.init</code> function and provide an experiment name. An experiment is a logical unit in Vertex that can contain several runs. On the Vertex.ai page you can find them on the left under the <code>MODEL DEVELOPMENT</code> tab (Figure 2).</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 318px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/fbraza-github-pages/static/fe9541f67d0524d5173e3790448c6962/cd2d9/2023-05-06-overview-vertexai-figure-02.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 63.671875%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABX0lEQVR42q1T2W7CMBDM/7/2L/ohkAAl0FCSpq0EOWloTpHTjqe2EYRKFbQSK03WWdvj2fVawZ1NEZ/lcgFV1TCbPcF1HSx0Hbq+gGVamM/n0CYTjEcjGKsVKKXnzYwx6auqQlmWA2HgB5zI43CRJAk8z4Pv+wjDHYIghMfHm80G0X7/q6q2bVHX9UB495SLooBpmhAZUEJgWyZW6zXWPCaUC+v7/pziTUIh1w/8U2XwGQY8xS0cZyvLcIr/SWEYE/RCAV/fkCOEECZVHeOUe0KH+WtQplaFomaYmgfk3Ed5D9tr8C7gC7Sw3RZORJBVDGl5HYr+ViGMKR4ed8h4YJ9TGB8VtJeD9M8c+mvJD2mRc0Kx5hoU223Q8XTqjuHQMJQc4p/0Ay7nb0FeimjWPM9kUQnpEMcx0jRDkqayL7uO/K9tRKcbhoHTjavqGJqmYTRW5QuJvpIfL+Mm4T3tGyhB7zadsQYjAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"experiments.png\" title=\"Where to find experiments\" src=\"/fbraza-github-pages/static/fe9541f67d0524d5173e3790448c6962/cd2d9/2023-05-06-overview-vertexai-figure-02.png\" srcset=\"/fbraza-github-pages/static/fe9541f67d0524d5173e3790448c6962/6f3f2/2023-05-06-overview-vertexai-figure-02.png 256w,\n/fbraza-github-pages/static/fe9541f67d0524d5173e3790448c6962/cd2d9/2023-05-06-overview-vertexai-figure-02.png 318w\" sizes=\"(max-width: 318px) 100vw, 318px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span></p>\n<p><em>Figure 2</em> : The MODEL DEVELOPMENT tab</p>\n<p>Next, you just need to use the context manager <code>aiplatform.start_run()</code> to which you need to pass an identifier (<code>run</code>). Inside the context manager scope implement your logic for model training and metrics computations. If you need to log the model parameters use the <code>log_params()</code> function. The parameters need to be of type <code>dict</code>. Similarly you can use <code>log_metrics()</code> to log your model metrics. Here again provide the metrics as a dictionnary. Once your model is trained you can find the metrics on your vertex environment. First click on <code>Experiments</code> to list your own experiments (Figure 3).</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1024px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/fbraza-github-pages/static/1adf9d7d11ff3975fbd47a3d7c2ffb4a/52c1b/2023-05-06-overview-vertexai-figure-03.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 42.578125%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA6ElEQVR42qWS227DIBBE+f+fbCO1qhI34WIwd7Y7m5paeajUFGmEh10OaLByPlNtjUotVEqhGCM19mOMp6RueqUQAm1bkNl7/z/g9fpJWjta1x8ogM8OZR0AYy4AVmv9M6j1IVLa4mZJ8hMYzzlngcJDe23XsQahP5dKPkRSH4unm838KJV675S8o9PrC12WRTw2O+dEO8xaOz2UUpL1wmD1fkkUYudL30PtrUgzssTAGjwgUudD8G2MnQ/BW+dh6u0c+YZVcpwN36AjQGs9vTFGdOybwHWrnEH/9VeQjHJ68PecdyFnzF9w7sP1AfqEfgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"list_experiments.png\" title=\"Experiments list\" src=\"/fbraza-github-pages/static/1adf9d7d11ff3975fbd47a3d7c2ffb4a/2bef9/2023-05-06-overview-vertexai-figure-03.png\" srcset=\"/fbraza-github-pages/static/1adf9d7d11ff3975fbd47a3d7c2ffb4a/6f3f2/2023-05-06-overview-vertexai-figure-03.png 256w,\n/fbraza-github-pages/static/1adf9d7d11ff3975fbd47a3d7c2ffb4a/01e7c/2023-05-06-overview-vertexai-figure-03.png 512w,\n/fbraza-github-pages/static/1adf9d7d11ff3975fbd47a3d7c2ffb4a/2bef9/2023-05-06-overview-vertexai-figure-03.png 1024w,\n/fbraza-github-pages/static/1adf9d7d11ff3975fbd47a3d7c2ffb4a/71c1d/2023-05-06-overview-vertexai-figure-03.png 1536w,\n/fbraza-github-pages/static/1adf9d7d11ff3975fbd47a3d7c2ffb4a/a878e/2023-05-06-overview-vertexai-figure-03.png 2048w,\n/fbraza-github-pages/static/1adf9d7d11ff3975fbd47a3d7c2ffb4a/52c1b/2023-05-06-overview-vertexai-figure-03.png 2163w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span></p>\n<p><em>Figure 3</em> : The Experiments tab</p>\n<p>Finally, click on one of them to list your runs with all your logged metrics and parameters (Figure 4).</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1024px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/fbraza-github-pages/static/98c8c26a5634dcf698945cb21b26689d/d0002/2023-05-06-overview-vertexai-figure-04.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 58.59375%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+ElEQVR42pVTixKDIAzj/391D2Aqb+gadjCc4t08c9XShsSKUHolJSUZGynEQlQK3/8j50LGJRIh5prA1eLsmq2PeaG1Imstw1EIocN7f4A7yfU15yqEki+6KyY0piZAjvgtPpLPNowxknhpw4QogMLYyXLOO6SUOmZ5QEhW99Sw6SshFEHB2PzbdAbUVYVy8fRQkGwrWVOYUjw0oeEq97G8OLpL3y2379GUjXG0efZcCZc1sOUPIRJtyleWd7m8Vyv06ukmMXJDgS1jymcKZxuAZITQsKy+lmcKrya9sxz5pDjfCsajxO8lT45Zri62basw/A8jwt0bOgywb6nfWk0AAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"log_metrics.png\" title=\"Run metrics\" src=\"/fbraza-github-pages/static/98c8c26a5634dcf698945cb21b26689d/2bef9/2023-05-06-overview-vertexai-figure-04.png\" srcset=\"/fbraza-github-pages/static/98c8c26a5634dcf698945cb21b26689d/6f3f2/2023-05-06-overview-vertexai-figure-04.png 256w,\n/fbraza-github-pages/static/98c8c26a5634dcf698945cb21b26689d/01e7c/2023-05-06-overview-vertexai-figure-04.png 512w,\n/fbraza-github-pages/static/98c8c26a5634dcf698945cb21b26689d/2bef9/2023-05-06-overview-vertexai-figure-04.png 1024w,\n/fbraza-github-pages/static/98c8c26a5634dcf698945cb21b26689d/71c1d/2023-05-06-overview-vertexai-figure-04.png 1536w,\n/fbraza-github-pages/static/98c8c26a5634dcf698945cb21b26689d/a878e/2023-05-06-overview-vertexai-figure-04.png 2048w,\n/fbraza-github-pages/static/98c8c26a5634dcf698945cb21b26689d/d0002/2023-05-06-overview-vertexai-figure-04.png 2147w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span></p>\n<p><em>Figure 4</em> : An example run with logged metrics</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>It has been a long time since I wanted to summarize the main aspects of Vertex.ai. I use vertex to train, track models and generate formatted reports that include metrics and other in-house scores. It is very pleasant to use. As a solution for MLOps it does the job but remains limited when compared to other solution like <a href=\"https://mlflow.org/\">MLFlow</a> or <a href=\"https://wandb.ai/site\">Weights &#x26; Biases</a>. Coding using the Kubeflow python sdk can be overwhelming at first especially if you do not take the time to refactor your components. Make them short, it will help for sure.</p>\n<h2 id=\"supplemental-data\" style=\"position:relative;\"><a href=\"#supplemental-data\" aria-label=\"supplemental data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Supplemental data</h2>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token comment\"># PIPELINE DEFINITION</span>\n<span class=\"token comment\"># Name: name_of_pipeline</span>\n<span class=\"token comment\"># Inputs:</span>\n<span class=\"token comment\">#    my_input: str</span>\n<span class=\"token comment\"># Outputs:</span>\n<span class=\"token comment\">#    my_output: system.Dataset</span>\n<span class=\"token key atrule\">components</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">comp-name_of_pipeline</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">executorLabel</span><span class=\"token punctuation\">:</span> name_of_pipeline\n    <span class=\"token key atrule\">inputDefinitions</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">parameters</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">my_input</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">parameterType</span><span class=\"token punctuation\">:</span> STRING\n    <span class=\"token key atrule\">outputDefinitions</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">artifacts</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">my_outputs</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">artifactType</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">schemaTitle</span><span class=\"token punctuation\">:</span> system.Dataset\n            <span class=\"token key atrule\">schemaVersion</span><span class=\"token punctuation\">:</span> 0.0.1\n<span class=\"token key atrule\">deploymentSpec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">executors</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">exec-name_of_pipeline</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">container</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">args</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>executor_input\n        <span class=\"token punctuation\">-</span> <span class=\"token string\">'{{$}}'</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>function_to_execute\n        <span class=\"token punctuation\">-</span> name_of_pipeline\n        <span class=\"token key atrule\">command</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> sh\n        <span class=\"token punctuation\">-</span> <span class=\"token punctuation\">-</span>c\n        <span class=\"token punctuation\">-</span> \"\\nif <span class=\"token tag\">!</span> <span class=\"token punctuation\">[</span> <span class=\"token punctuation\">-</span>x \\\"$(command <span class=\"token punctuation\">-</span>v pip)\\\" <span class=\"token punctuation\">]</span>; then\\n    python3 <span class=\"token punctuation\">-</span>m ensurepip <span class=\"token punctuation\">|</span><span class=\"token punctuation\">|</span>\\\n          \\ python3 <span class=\"token punctuation\">-</span>m ensurepip <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>user <span class=\"token punctuation\">|</span><span class=\"token punctuation\">|</span> apt<span class=\"token punctuation\">-</span>get install python3<span class=\"token punctuation\">-</span>pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1\\\n          \\ python3 <span class=\"token punctuation\">-</span>m pip install <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>quiet     <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>no<span class=\"token punctuation\">-</span>warn<span class=\"token punctuation\">-</span>script<span class=\"token punctuation\">-</span>location 'pandas'\\\n          \\ 'kfp==2.0.1' <span class=\"token important\">&amp;&amp;</span> \\\"$0\\\" \\\"$@\\\"\\n\"\n        <span class=\"token punctuation\">-</span> sh\n        <span class=\"token punctuation\">-</span> <span class=\"token punctuation\">-</span>ec\n        <span class=\"token punctuation\">-</span> 'program_path=$(mktemp <span class=\"token punctuation\">-</span>d)\n\n          printf \"%s\" \"$0\" <span class=\"token punctuation\">></span> \"$program_path/ephemeral_component.py\"\n\n          python3 <span class=\"token punctuation\">-</span>m kfp.components.executor_main <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>component_module_path \"$program_path/ephemeral_component.py\" \"$@\"\n          '\n        <span class=\"token punctuation\">-</span> <span class=\"token string\">\"components code is here\"</span>\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> eu.gcr.io/sensav2/suntraining/sunpy\n<span class=\"token key atrule\">pipelineInfo</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> name_of_pipeline\n<span class=\"token key atrule\">root</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">dag</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">outputs</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">artifacts</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">my_outputs</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">artifactSelectors</span><span class=\"token punctuation\">:</span>\n          <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">outputArtifactKey</span><span class=\"token punctuation\">:</span> name_of_pipeline\n            <span class=\"token key atrule\">producerSubtask</span><span class=\"token punctuation\">:</span> name_of_pipeline\n    <span class=\"token key atrule\">tasks</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">name_of_pipeline</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">cachingOptions</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">enableCache</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n        <span class=\"token key atrule\">componentRef</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> comp<span class=\"token punctuation\">-</span>name_of_pipeline\n        <span class=\"token key atrule\">inputs</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">parameters</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">my_input</span><span class=\"token punctuation\">:</span>\n              <span class=\"token key atrule\">componentInputParameter</span><span class=\"token punctuation\">:</span> my_input\n        <span class=\"token key atrule\">taskInfo</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> name_of_pipeline\n  <span class=\"token key atrule\">inputDefinitions</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">parameters</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">my_input</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">parameterType</span><span class=\"token punctuation\">:</span> STRING\n  <span class=\"token key atrule\">outputDefinitions</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">artifacts</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">my_outputs</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">artifactType</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">schemaTitle</span><span class=\"token punctuation\">:</span> system.Dataset\n          <span class=\"token key atrule\">schemaVersion</span><span class=\"token punctuation\">:</span> 0.0.1\n<span class=\"token key atrule\">schemaVersion</span><span class=\"token punctuation\">:</span> 2.1.0\n<span class=\"token key atrule\">sdkVersion</span><span class=\"token punctuation\">:</span> kfp<span class=\"token punctuation\">-</span>2.0.1</code></pre></div>","frontmatter":{"title":"An overview of Vertex.ai","date":"May 06, 2023","tags":["Python","Google Cloud"]}}},"pageContext":{"id":"ded8f7c9-28bc-5ae8-826d-07922cf569d8","fields__slug":"/overview-of-vertexai/","__params":{"fields__slug":"overview-of-vertexai"}}},"staticQueryHashes":["2750728228"],"slicesMap":{}}